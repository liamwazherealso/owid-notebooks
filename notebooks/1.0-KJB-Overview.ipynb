{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview Notebook\n",
    "## Dataset questions \n",
    "    -count of dataset directories\n",
    "    -files within each dataset directory\n",
    "        * does each directory have 1 dataset as a csv a README.md and a datapackage.json\n",
    "        * A README.md containing the datasets description\n",
    "        * if not what is in each directory\n",
    "        * recursively search each directory if it contains subirectories and determine its content    \n",
    "    - return JSON object for directories that do not have a 1 csv 1 README.md and a datapackage.json in it.\n",
    "    -for sets make structural checks for analysis\n",
    "        * Organize the dtypes of each dataset into a dataframe\n",
    "        * check shape\n",
    "        * if dataset contains worldwide data check to see which countries are generally underrepresented\n",
    "\n",
    "     -for checking all datasets\n",
    "        * check to see the primary, secondary and tertiary modes range for all sets (maybe represent this as a histogram)\n",
    "     -for handling mixed datatypes at the series level\n",
    "         *TBD\n",
    "      for datapackage.jsons\n",
    "          *Create a csv containing source links,id,name published by\n",
    "\n",
    "## Analysis\n",
    "\n",
    "    -Create sentiment analysis of datasets determining the best and worse years based on a scoring system\n",
    "    - exploratory analysis \n",
    "        * TBD\n",
    "    - linear plot of the score over the primary secondary and tertiary mode Year ranges\n",
    "    - find a way to weigh each dataset according to impact level\n",
    "        * A library may exist for this\n",
    "     - cleaning process\n",
    "         * TBD\n",
    "### Primary Extraction\n",
    "## ML Processing\n",
    "    -TBD\n",
    "\n",
    "#### datapackage.json example{\n",
    "name:\"Working Hours Data - Huberman & Minns (2007)\"\n",
    "title:\"Working Hours Data - Huberman & Minns (2007)\"\n",
    "id:234\n",
    "description:\"\"\n",
    "name:\"Huberman & Minns (2007)\"\n",
    "link:\"The paper is online here: http://www.sciencedirect.com/science/article/pii/S0014498307000058\"\n",
    "dataPublishedBy:\"Huberman & Minns (2007) – The times they are not changin’: Days and hours of work in Old and New Worlds, 1870–2000. Explorations in Economic History, 44(4):538–567.\"\n",
    "0:\"Working Hours\"\n",
    "path:\"Working Hours Data - Huberman & Minns (2007).csv\"\n",
    "name:\"Entity\"\n",
    "type:\"string\"\n",
    "name:\"Year\"\n",
    "type:\"year\"\n",
    "name:\"Full-time production workers (male and female) in non-agricultural activities (Weekly Work Hours) (Huberman & Minns (2007)) \"\n",
    "type:\"any\"\n",
    "description:\"Data refers to full-time production workers (male and female) in non-agricultural activities. Only for 2000 Huberman & Minns give estimates for both genders separately. The average of the two measures have been calculated and visualised. The original sources are: 1870–1913: Huberman (2004); 1929–1938, ILO (1934–38), except for Canada (Ostry and Zaidi, 1972), U.S. (Jones, 1963 and Owen, 1988), and for Australia (Butlin, 1977); the values for Spain in 1938 are for 1936; 1950–1980: ILO (1950–80), except for U.S. (McGratten and Rogerson, 2004), and Australia ( Butlin, 1977); 1980–2000: ILO (2005), except for U.S. (McGratten and Rogerson, 2004), Canada (Heisz and LaRochelle-Côté, 2003), and Denmark (Eurostat, 1995).\"\n",
    "owidDisplaySettings:\"{\"name\": \"Weekly Work Hours – Full-time production workers in non-agricultural activities\", \"includeInTable\": true}\"\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.visualization.visualize import *\n",
    "from src.data.make_dataset import *\n",
    "import subprocess\n",
    "import json  \n",
    "\n",
    "#DATASETS = \"../owid-datasets/datasets/*\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "datasets 993\n"
     ]
    }
   ],
   "source": [
    "dataset_dir_cnt=0\n",
    "for i in glob(DATASETS):\n",
    "    dataset_dir_cnt+=1\n",
    "print(dataset_dir_cnt)\n",
    "for dataset in glob(DATASETS+'*'):\n",
    "    if len(os.listdir(dataset))!=3:\n",
    "           print(dataset.split('/')[-1],len(os.listdir(dataset)))\n",
    "# get json return json\n",
    "# look for links in load return links as comma separated string\n",
    "# main will loop through ds's get json look for links create csk\n",
    "def get_datapackages_json(dataset):\n",
    "    with open(\"{}/datapackage.json\".format(os.path.abspath(dataset)), \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "    return data\n",
    "    \n",
    "def parse_dict(string, _dict, strings=None):\n",
    "    \"\"\"\n",
    "    returns a string of http links within a dictionary, Links are separated by commas\n",
    "    \n",
    "    \"\"\"\n",
    "    if not strings:\n",
    "        strings=''\n",
    "    if not isinstance(_dict,dict):\n",
    "        return strings\n",
    "    else:\n",
    "        for key, value in _dict.items():\n",
    "            if string in key:\n",
    "                strings+=',{}'.format(key)\n",
    "            if isinstance(value,str):\n",
    "                if string in values:\n",
    "                    strings+=','+value\n",
    "            else:\n",
    "                parse_dict(string, value,strings)\n",
    "    \n",
    "def generate_datapackage_csv(datasets):\n",
    "    headers=['id','title','links']\n",
    "    body=[]\n",
    "    for dataset in glob(datasets+'/*'):\n",
    "        data=get_datapackages_json(dataset)\n",
    "        http_list= parse_dict('http', data)\n",
    "        dataset_id=data # do stuff\n",
    "        dataset_title =data # do stuff\n",
    "        body.append([dataset_id,dataset_title,http_list])\n",
    "    df=pd.DataFrame(columns=headers,data=body)\n",
    "    return df\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_dict('oo','p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'name': 'Ozone hole area and concentration - ...</td>\n",
       "      <td>{'name': 'Ozone hole area and concentration - ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'name': 'CO2 emissions factors of energy sour...</td>\n",
       "      <td>{'name': 'CO2 emissions factors of energy sour...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'name': 'CO2 intensity- (kgCO2/2011-$ PPP)- W...</td>\n",
       "      <td>{'name': 'CO2 intensity- (kgCO2/2011-$ PPP)- W...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'name': 'Prevalence of weight categories in f...</td>\n",
       "      <td>{'name': 'Prevalence of weight categories in f...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'name': 'Political Regimes — V-Dem (2019)', '...</td>\n",
       "      <td>{'name': 'Political Regimes — V-Dem (2019)', '...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>{'name': 'Poverty headcount at $1.90 a day (20...</td>\n",
       "      <td>{'name': 'Poverty headcount at $1.90 a day (20...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>{'name': 'Life expectancy - Riley (2005), Clio...</td>\n",
       "      <td>{'name': 'Life expectancy - Riley (2005), Clio...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>{'name': 'Cumulative share of marriages ending...</td>\n",
       "      <td>{'name': 'Cumulative share of marriages ending...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>{'name': 'Population Estimates and Projection ...</td>\n",
       "      <td>{'name': 'Population Estimates and Projection ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>{'name': 'Genome Production Costing - GSP', 't...</td>\n",
       "      <td>{'name': 'Genome Production Costing - GSP', 't...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   id  \\\n",
       "0   {'name': 'Ozone hole area and concentration - ...   \n",
       "1   {'name': 'CO2 emissions factors of energy sour...   \n",
       "2   {'name': 'CO2 intensity- (kgCO2/2011-$ PPP)- W...   \n",
       "3   {'name': 'Prevalence of weight categories in f...   \n",
       "4   {'name': 'Political Regimes — V-Dem (2019)', '...   \n",
       "..                                                ...   \n",
       "85  {'name': 'Poverty headcount at $1.90 a day (20...   \n",
       "86  {'name': 'Life expectancy - Riley (2005), Clio...   \n",
       "87  {'name': 'Cumulative share of marriages ending...   \n",
       "88  {'name': 'Population Estimates and Projection ...   \n",
       "89  {'name': 'Genome Production Costing - GSP', 't...   \n",
       "\n",
       "                                                title links  \n",
       "0   {'name': 'Ozone hole area and concentration - ...  None  \n",
       "1   {'name': 'CO2 emissions factors of energy sour...  None  \n",
       "2   {'name': 'CO2 intensity- (kgCO2/2011-$ PPP)- W...  None  \n",
       "3   {'name': 'Prevalence of weight categories in f...  None  \n",
       "4   {'name': 'Political Regimes — V-Dem (2019)', '...  None  \n",
       "..                                                ...   ...  \n",
       "85  {'name': 'Poverty headcount at $1.90 a day (20...  None  \n",
       "86  {'name': 'Life expectancy - Riley (2005), Clio...  None  \n",
       "87  {'name': 'Cumulative share of marriages ending...  None  \n",
       "88  {'name': 'Population Estimates and Projection ...  None  \n",
       "89  {'name': 'Genome Production Costing - GSP', 't...  None  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=generate_datapackage_csv(DATASETS)\n",
    "df.to_csv('p.csv')\n",
    "df[(df['links']!='None')].head(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kareem/Desktop/github_projects/owid_project_branches/kareem-analysis/owid-notebooks/owid-datasets/datasets/datasets/datasets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5298ccdfa44e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ds_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/github_projects/owid_project_branches/kareem-analysis/owid-notebooks/src/data/make_dataset.py\u001b[0m in \u001b[0;36mget_ds_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mds_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/github_projects/owid_project_branches/kareem-analysis/owid-notebooks/src/data/make_dataset.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m# consolidate code: dtype hierarchy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kareem/Desktop/github_projects/owid_project_branches/kareem-analysis/owid-notebooks/owid-datasets/datasets/datasets/datasets.csv'"
     ]
    }
   ],
   "source": [
    "ds_dict = get_ds_dict()\n",
    "df_cols = get_df_cols(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_uniq = get_col_to_uniq(df_cols)\n",
    "col_to_dtype = get_col_to_dtype(col_to_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = {'Forest Transition Phase': 'object'}\n",
    "\n",
    "# don't have to create a blank df with a do_while\n",
    "do_while = 0\n",
    "for ds in glob(DATASETS):\n",
    "    do_while += 1\n",
    "    if do_while == 1:\n",
    "        ds = os.path.basename(ds)\n",
    "        df = load_data(ds)\n",
    "        df = df.set_index([\"Entity\", \"Year\"])\n",
    "        df = apply_data_hierarchy(df, col_to_dtype, overwrite=overwrite)\n",
    "        continue\n",
    "        \n",
    "    ds = os.path.basename(ds)\n",
    "    df2 = load_data(ds)\n",
    "    df2 = df2.set_index([\"Entity\", \"Year\"])\n",
    "    \n",
    "    overwrite['ds_name'] = ds\n",
    "    df2 = apply_data_hierarchy(df2, col_to_dtype, overwrite=overwrite)\n",
    "    \n",
    "    try:\n",
    "        df = pd.merge(df, df2, how='left', on=[\"Entity\", \"Year\"])\n",
    "    except ValueError:\n",
    "        print(\"MergeError:\", ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = load_data('Cumulative share of marriages ending in divorce (England and Wales, UK ONS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_df = apply_data_hierarchy(df2, col_to_dtype, overwrite={\"Year\": \"object\"})\n",
    "dataset_all_df=pd.merge(df, retry_df, how='left', on=[\"Entity\", \"Year\"])\n",
    "dataset_all_df.to_csv('../models/data_set_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
